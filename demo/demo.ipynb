{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1c7b57-170b-4819-90ce-63b176a6d275",
   "metadata": {},
   "source": [
    "<h2> Neural Network Music Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11ee43-0593-4866-9974-34733ccd776c",
   "metadata": {},
   "source": [
    "**Abstract**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb311d9-a6c3-48fb-87ee-f3dfdb0fa758",
   "metadata": {},
   "source": [
    "With this project, we set out to create a neural network which could somehow \"listen\" to music, learn compositional patterns,<br>\n",
    "and generate its own unique music after proper training. To accomplish this, we used MIDI data, which is a digital representation<br>\n",
    "of music containing distinct events that communicate information such as note pitch and velocity. The goal was to use this data<br>\n",
    "to get a neural network to generalize harmonic relationships between notes (in other words, determine what sounds \"good\") as well<br>\n",
    "as proper musical timing. Over the course of our work, we created two types of networks: a Recurrent Neural Network and a <br>\n",
    "General Adversarial Network, in order to see how the two options compare in music generation. Future work on this may include <br>\n",
    "adding in some method of establishing the basic rules of music theory for the network to abide by."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d144e75-b64f-4e4c-93fc-c30fecd87def",
   "metadata": {},
   "source": [
    "<h4> Recurrent Neural Networks vs. General Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b181b7-da9f-4099-a8fa-6f2576f225f5",
   "metadata": {},
   "source": [
    "Below we present two deep learning methods for generating music: *Recurrent Neural Networks* (RNN) and *General Adversarial Networks* (GAN). We will explore the basic concepts of RNN and GNN to generate music. <br> <br>\n",
    "But before we begin, we will import some helpul packages and tools to accomplish these tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd833fd-046c-41aa-bab9-47b142373c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "#MIIDI file processing tools\n",
    "import pretty_midi\n",
    "import seaborn as sns\n",
    "# import fluidsynth\n",
    "\n",
    "#Neural Network Tools\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.xception import preprocess_input, decode_predictions\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "#Plotting Tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa666ef-fb6a-4c3e-bef5-4f8dd056412d",
   "metadata": {},
   "source": [
    "<h3> Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f37d0b9-3c1b-4648-8192-bb6ede75ece2",
   "metadata": {},
   "source": [
    "- explain what the RNN is trying to do (how it works) (Louis)\n",
    "                - BRIEF intro to RNN structure\n",
    "                        - example midi file played\n",
    "                        - explain wHAT we are trying to do (i.e the losses we are trying to minimize\n",
    "                                                                                        (pitch/step/duration)\n",
    "                - load up the trained model\n",
    "                - show the structure of the trained model\n",
    "                - generate a new song with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ef04ac-729d-4e0d-b176-8f291bed2a22",
   "metadata": {},
   "source": [
    "<h3> General Adversarial Neural Network (GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003fc6f8-daa3-4652-9e24-79aed5c5bded",
   "metadata": {},
   "source": [
    "To create a General Adversarial Neural Network, we will first create 2 different networks and combine them into one. We will contruct: a **generator** and a **discriminator**. <br><br>\n",
    "The *generator* is designed to take random noise (an array of random values from -1 to 1) and try to generate an appropriate image. The *discriminator* is trained on both the real and fake (generated) images and trys to label the images as fake (generated images) or real (actual images). <br><br> When we combine the models, the networks will be forced to trade-off loss on one another. So, when the discriminator becomes good at determining between real and fake images, the generator is forced to make more real-looking images. When the generator makes better-looking images, the discriminator must learn to make more careful decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a601dc-e668-4283-8c28-b2fdd27f569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools for NN and image processing\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.xception import preprocess_input, decode_predictions\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becf225b-be24-4cc0-9e04-34635fe1ee87",
   "metadata": {},
   "source": [
    "But how do we make music by generating images? We will need to do some unique data transformations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a314d4fb-7dd8-43d1-87ba-a9c01638f638",
   "metadata": {},
   "source": [
    "<h5> Midi Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6c666f-f67a-45e5-b262-b65464d7ee5b",
   "metadata": {},
   "source": [
    "To be able to generate music with images, we first have to transform our training music (midi files) to images! To do this, we use the helper functions found [here](https://github.com/mathigatti/midi2img).  <br><br>\n",
    "Using the python script midi2img.py, we converted midi files into a 106 X 106 image, where the **each row represents a note** (from 21 (A0) to 127 (G9)) and **each column represents the timestep of the song** (1/4 beat per pixel). <br><br>\n",
    "To understand this more clearly, let's load up some example images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aeedf9-fe7e-42ba-bfce-3a68e9c1cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_image(img_path):\n",
    "    img = image.load_img(img_path, color_mode=\"grayscale\", target_size=(106,106))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f0a4b7-4dee-4a7c-92ce-d88f5583c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in midi images\n",
    "exampleImages = np.concatenate([grab_image('./example_midi_images/%s'%(filename)) for filename in os.listdir(\"./example_midi_images/\") if filename.endswith(\".png\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fc863-b1e8-40d3-af2e-7b0715d1a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(exampleImages)):\n",
    "    plt.imshow(image.array_to_img(exampleImages[i,:,:,:]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9282bf11-3375-4622-9640-1500c3a464ce",
   "metadata": {},
   "source": [
    "**To generate new midi images, we created a GAN network consisting of a generator (to create the new midi images) and discriminator (to determine which midi images are real or fake).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6999887c-bc48-4cbd-8a47-660bce9f7a7e",
   "metadata": {},
   "source": [
    "To train the GAN on your own, uncomment the following cells. If you have limited time and want to load a pretrained model, proceed to the cells down below! The following trains on the example images -- if you wish to see better output, add more midi images to the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb0ee7-7caa-4f7d-8866-391e3bcf3bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#get data ready to train\n",
    "x_train = exampleImages\n",
    "#all real images have a 1 label\n",
    "y_train = np.ones((exampleImages.shape[0], 1))\n",
    "img_rows = x_train.shape[1]\n",
    "img_cols = x_train.shape[2]\n",
    "channels = x_train.shape[3]\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "num_classes = len(np.unique(y_train))+1\n",
    "latent_dim = 100\n",
    "img_shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc102cd-74c5-4182-b580-a6e971e87e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Generator\n",
    "noise = keras.layers.Input(shape=(latent_dim,))\n",
    "\n",
    "label = keras.layers.Input(shape=(1,), dtype='int32')\n",
    "label_embedding = keras.layers.Embedding(num_classes, latent_dim)(label)\n",
    "label_embedding = keras.layers.Flatten()(label_embedding)\n",
    "\n",
    "generator_input = keras.layers.multiply([noise, label_embedding])\n",
    "\n",
    "generator_hidden = keras.layers.Dense(128 * 52 * 52, # Reshapable\n",
    "                                      activation='relu',\n",
    "                                      input_dim=latent_dim)(generator_input)\n",
    "generator_hidden = keras.layers.Reshape((52, 52, 128))(generator_hidden)\n",
    "generator_hidden = keras.layers.BatchNormalization(momentum=0.8)(generator_hidden)\n",
    "generator_hidden = keras.layers.Conv2DTranspose(64,\n",
    "                                                kernel_size=(4,4),\n",
    "                                                strides=(2,2),\n",
    "                                                activation='relu')(generator_hidden)\n",
    "\n",
    "generator_hidden = keras.layers.BatchNormalization(momentum=0.8)(generator_hidden)\n",
    "g_image = keras.layers.Conv2DTranspose(channels, \n",
    "                                       kernel_size=(7,7),\n",
    "                                       padding='same',\n",
    "                                       activation='tanh')(generator_hidden)\n",
    "\n",
    "generator = keras.Model([noise, label], g_image, name='Generator')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c0c72-32e2-4b2d-ba61-c2520db4e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Discriminator\n",
    "d_image = keras.layers.Input(shape=img_shape)\n",
    "\n",
    "# Hidden layers\n",
    "discriminator_hidden = keras.layers.Conv2D(16, kernel_size=(3,3), strides=(2,2),padding='same')(d_image)\n",
    "discriminator_hidden = keras.layers.LeakyReLU(alpha=0.2)(discriminator_hidden)\n",
    "discriminator_hidden = keras.layers.Dropout(0.25)(discriminator_hidden)\n",
    "discriminator_hidden = keras.layers.Conv2D(16, kernel_size=(3,3), strides=(2,2), padding='same')(discriminator_hidden)\n",
    "#discriminator_hidden = keras.layers.ZeroPadding2D(padding=((0,1),(0,1)))(discriminator_hidden)\n",
    "discriminator_hidden = keras.layers.LeakyReLU(alpha=0.2)(discriminator_hidden)\n",
    "discriminator_hidden = keras.layers.Dropout(0.25)(discriminator_hidden)\n",
    "discriminator_hidden = keras.layers.LeakyReLU(alpha=0.2)(discriminator_hidden)\n",
    "#discriminator_hidden = keras.layers.Dropout(0.25)(discriminator_hidden)\n",
    "discriminator_hidden = keras.layers.Flatten()(discriminator_hidden)\n",
    "#discriminator_hidden = keras.layers.BatchNormalization(momentum=0.8)(discriminator_hidden)\n",
    "discriminator_hidden = keras.layers.Dropout(0.25)(discriminator_hidden)\n",
    "\n",
    "target_label = keras.layers.Dense(1, activation=keras.activations.sigmoid)(discriminator_hidden)\n",
    "\n",
    "# Finalize the model\n",
    "discriminator = keras.Model(d_image, target_label, name=\"Discriminator\")\n",
    "discriminator.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "                      optimizer=keras.optimizers.Adam(0.0002,0.5),  #ADJUST ME!\n",
    "                      metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad2b82-13b5-4d00-b80e-72a52f3e342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Combined model...\n",
    "\n",
    "combined_model_inputs = [keras.layers.Input(shape=x[1:]) for x in generator.input_shape]\n",
    "generator_output = generator(combined_model_inputs)\n",
    "# Turn off learning for discriminator...\n",
    "discriminator.trainable = False\n",
    "\n",
    "target_label = discriminator(generator_output) # g_image = generator output layer\n",
    "# Combined model now takes generator inputs and has discriminator outputs...\n",
    "combined = keras.Model(combined_model_inputs,target_label)\n",
    "combined.summary\n",
    "combined.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "                 optimizer=keras.optimizers.Adam(0.0002,0.5))   #ADJUST ME!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018afd38-6282-407c-b793-f1fe917ea5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Training Parameters\n",
    "batch_size = 4  #ADJUST ME!\n",
    "half_batch_size = int(batch_size/2)\n",
    "\n",
    "# Keep track of discriminatory loss[0], disciminator accuracy[1] and generator loss[2]\n",
    "history = [[],[],[]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d269d-bc28-4553-94ff-800ce8206c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "# Perform some number of batches...\n",
    "batches = 20    #ADJUST ME!\n",
    "for batch in range(batches):\n",
    " # Training the DISCRIMINATOR\n",
    " # REAL images\n",
    " idx = np.random.randint(0, x_train.shape[0], half_batch_size)\n",
    " real_images = x_train[idx]\n",
    " image_labels = y_train[idx]\n",
    "\n",
    " # FAKE images\n",
    " noise = np.random.normal(-1, 1, (half_batch_size, 100))\n",
    " sampled_labels = np.ones((half_batch_size, 1))\n",
    " generated_images = generator.predict([noise, sampled_labels])\n",
    "\n",
    " # Assign the fake images to the \"fake class\"\n",
    " fake_labels = np.zeros(half_batch_size).reshape(-1, 1)\n",
    "\n",
    " # Train the discriminator - Two groups\n",
    " d_loss_real = discriminator.train_on_batch(real_images, image_labels)\n",
    " d_loss_fake = discriminator.train_on_batch(generated_images, fake_labels)\n",
    " \n",
    " # \"overall\" loss/accuracy for the discriminator  \n",
    " d_loss = np.add(d_loss_real, d_loss_fake) / 2.0\n",
    "\n",
    " # Training the generator...\n",
    " noise = np.random.normal(-1, 1, (batch_size, 100))\n",
    "\n",
    " # calling these real images - try to fake-out the discriminator.\n",
    " #Setting the target as if the generated images were real (1)\n",
    " sampled_labels = np.ones((batch_size, 1))\n",
    " \n",
    " # Train the generator\n",
    " g_loss = combined.train_on_batch([noise, sampled_labels], sampled_labels)\n",
    " \n",
    " history[0] += [d_loss[0]]\n",
    " history[1] += [d_loss[1]]\n",
    " history[2] += [g_loss]\n",
    "    \n",
    " # progress indicator\n",
    " print(\"\\r%d [Disc. Loss: %f, Disc. Accuracy: %.2f%%] [Gen. Loss: %f]\"%(batch, d_loss[0], 100*d_loss[1], g_loss), end='')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec52d5-da5e-415d-bfcd-cb252b345112",
   "metadata": {},
   "source": [
    "<h5> Training the GAN Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19dbb87-5ae0-445e-96ba-d65e7f3405fd",
   "metadata": {},
   "source": [
    "To train the model, we loaded 563 of the midi images into an array, and we assigned every single image in our training set a \"real image\" label of 1. <br><br>\n",
    "We trained the model over **300** batches.<br> <br>\n",
    "For every batch:\n",
    " 1. We generated fake images using our generator by giving it random noise (an array of random values from -1 to 1) and the \"real\" class label of 1.\n",
    " \n",
    " 2. Once our fake midi images were generated, we assigned a \"fake\" class label of 0 for each image that we generated.\n",
    " \n",
    " 3. We trained the discriminator on the real images with the real labels and the fake images with the fake labels.\n",
    " \n",
    " 4. Finally, we trained the generator with random noise and a target label of 1 (as if the images are real) as input  to fake off the discriminator. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1914a8-429c-43fe-9ec5-4e3bb0f0f652",
   "metadata": {},
   "source": [
    "<h5> The Trained GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44625750-5369-4570-bfa3-71397c24b737",
   "metadata": {},
   "source": [
    "After training, we now have a model that is capable of generating new images from vectors of random noise.\n",
    "Because training takes several minutes, we will load an already trained model to demonstrate the midi image creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12811c01-0680-4374-a88a-c396d01952f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O trained_GAN.zip https://www.dropbox.com/s/qr9b3p1xfjkqzlw/trained_GAN.zip?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbafc53-43f0-463b-8f2e-1a926d4460fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip trained_GAN.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216146e-4ea5-4cd8-840f-12f6c5691bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./trained_GAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d84dc3-59c4-46df-98ba-07f73768402b",
   "metadata": {},
   "source": [
    "Lets take a look at the GAN's architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b834c9ac-b58a-4e02-ae4f-90ea9dad76f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see individual generator and discriminator layers/components \n",
    "#change expand_nested=False to expand_nested=True\n",
    "keras.utils.plot_model(model, show_shapes=True, expand_nested=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61292d25-8a1f-4640-84fe-c0193d78e026",
   "metadata": {},
   "source": [
    "<br><br>We just need the generator from the GAN to make some images, so lets load the generator layer into a seperate variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18221034-a0b0-4dc1-b2ce-198f29fbbef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = model.get_layer(name=\"Generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a3dc7-dee1-4eac-a4de-f7bdfb20cda9",
   "metadata": {},
   "source": [
    "We generate some images using some randome noise and a \"real\" class label..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e7f486-c5a9-4d44-88ff-1d217e272341",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "r, c = 10, 10\n",
    "noise = np.random.normal(-1, 1, (r * c, 100))\n",
    "print(noise.shape)\n",
    "sampled_labels = np.ones((100, 1))\n",
    "# Make some fakes!\n",
    "generated_images = generator.predict([noise, sampled_labels])\n",
    "# Plot them.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37120da3-637e-429b-86c7-eef9ee968397",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.array_to_img(generated_images[98,:,:,:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092fa49d-b838-4094-8a8d-060c167d78ab",
   "metadata": {},
   "source": [
    "and save the image..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91acac4-f031-4e57-a025-fa9e954f6a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageSave = image.array_to_img(generated_images[98,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7543456-3959-411c-b905-89703cce0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageSave = imageSave.save('composition.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a164682-65c3-46b1-853f-d9b07f0fe6aa",
   "metadata": {},
   "source": [
    "Now that we have generated and saved a midi image, we can use the other helper python script ([img2midi.py](https://github.com/mathigatti/midi2img)) to convert our fresh midi image into a new composition in the midi file format! If you have downloaded the helper functions, you can create this file by running the following command in the terminal: <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b95aa65-2983-477d-9c3b-fa93064fe1c3",
   "metadata": {},
   "source": [
    "```python img2midi.py composition.png```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
