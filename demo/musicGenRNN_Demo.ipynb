{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Intro\n",
        "\n",
        "In this demo, we construct a RNN that will learn to output piano music after being trained on a number of piano compositions in the MIDI file format."
      ],
      "metadata": {
        "id": "ew8ckTmN2YlD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVFMs6df56sk"
      },
      "outputs": [],
      "source": [
        "!sudo apt install -y fluidsynth\n",
        "!pip install --upgrade pyfluidsynth\n",
        "!pip install pretty_midi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R1jcXRe6bWn"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import datetime\n",
        "import fluidsynth\n",
        "import glob\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import pretty_midi\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "from typing import Dict, List, Optional, Sequence, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiMOcN_F62hK"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "#Sampling Rate for Audio Playback\n",
        "_SAMPLING_RATE = 16000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data\n",
        "\n",
        "This RNN can be trained on any number of MIDI files but, for the sake of brevity, we have uploaded 10 to use as data for this demo."
      ],
      "metadata": {
        "id": "XQNufhRM3IH0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxtcSGIS78hc"
      },
      "outputs": [],
      "source": [
        "#Import Data\n",
        "data_dir = pathlib.Path(\"data/example_midi_data\")\n",
        "!wget -O data.zip https://www.dropbox.com/s/vho3um36lpwmdri/example_midi_data.zip?dl=0\n",
        "!unzip -d data data.zip \n",
        "#data_dir = pathlib.Path(\"data/maestro-v2.0.0\")\n",
        "#if not data_dir.exists():\n",
        "  #tf.keras.utils.get_file('maestro-v2.0.0-midi.zip', origin='https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip', extract=True, cache_dir='.', cache_subdir='data')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ST6OyIBS83Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Bn1dTu48nfr"
      },
      "outputs": [],
      "source": [
        "filenames = glob.glob(str(data_dir/\"*.mid\"))\n",
        "print(\"Number of Files:\", len(filenames))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIF8TXPn84JE"
      },
      "outputs": [],
      "source": [
        "sample_file = filenames[1]\n",
        "print(sample_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5baMoyH39DW8"
      },
      "outputs": [],
      "source": [
        "pm = pretty_midi.PrettyMIDI(sample_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Functions\n",
        "\n",
        "There are a number of helper functions that are necessary for this method of neural net music generation to operate. Some coming from python packages and some coming courtesy of TensorFlow, they allow us to do things like convert MIDI files to our input data and vice-versa, display the music in piano roll form, generate notes with the trained model, among others."
      ],
      "metadata": {
        "id": "u_EF9HMI3syF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsFqP7kN9KUD"
      },
      "outputs": [],
      "source": [
        "#Display Audio\n",
        "def display_audio(pm: pretty_midi.PrettyMIDI, seconds = 30):\n",
        "  waveform = pm.fluidsynth(fs=_SAMPLING_RATE)\n",
        "  #Take a sample of the generated waveform to mitigate kernel resets\n",
        "  waveform_short = waveform[:seconds*_SAMPLING_RATE]\n",
        "  return display.Audio(waveform_short, rate=_SAMPLING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAkg1Nym9hcn"
      },
      "outputs": [],
      "source": [
        "#display_audio(pm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dyM_3EQFc36"
      },
      "outputs": [],
      "source": [
        "print(\"Number of Instruments:\", len(pm.instruments))\n",
        "instrument = pm.instruments[0]\n",
        "instrument_name = pretty_midi.program_to_instrument_name(instrument.program)\n",
        "print(\"Instrument Name:\", instrument_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY_49nd6FzPt"
      },
      "outputs": [],
      "source": [
        "for i, note in enumerate(instrument.notes[:10]):\n",
        "  note_name = pretty_midi.note_number_to_name(note.pitch)\n",
        "  duration = note.end - note.start\n",
        "  print(f\"{i}: pitch={note.pitch}, note_name={note_name},\" f\"duration={duration:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G6mbk16OARH"
      },
      "outputs": [],
      "source": [
        "#Midi To Notes\n",
        "def midi_to_notes(midi_file: str) -> pd.DataFrame:\n",
        "  pm = pretty_midi.PrettyMIDI(midi_file)\n",
        "  instrument = pm.instruments[0]\n",
        "  notes = collections.defaultdict(list)\n",
        "\n",
        "  #sort the notes by start time\n",
        "  sorted_notes = sorted(instrument.notes, key=lambda note: note.start)\n",
        "  prev_start = sorted_notes[0].start\n",
        "\n",
        "  for note in sorted_notes:\n",
        "    start = note.start\n",
        "    end = note.end\n",
        "    notes['pitch'].append(note.pitch)\n",
        "    notes['start'].append(start)\n",
        "    notes['end'].append(end)\n",
        "    notes['step'].append(start - prev_start)\n",
        "    notes['duration'].append(end - start)\n",
        "    prev_start = start\n",
        "\n",
        "  return pd.DataFrame({name: np.array(value) for name, value in notes.items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Format\n",
        "\n",
        "The MIDI-To-Notes converts a midi file into the data we are training the net on. It converts each note into three separate values; pitch, step, and duration. The pitch represents the frequency of the sound, the step is the amount of time that has passed since the last played note, and the duration is how long the note sounds out for."
      ],
      "metadata": {
        "id": "ZgClaBin4knt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfMZyUMSQ-lD"
      },
      "outputs": [],
      "source": [
        "#Notes by Pitch\n",
        "raw_notes = midi_to_notes(sample_file)\n",
        "raw_notes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEn7_6kBZLrO"
      },
      "outputs": [],
      "source": [
        "#Notes by Note\n",
        "get_note_names = np.vectorize(pretty_midi.note_number_to_name)\n",
        "sample_note_names = get_note_names(raw_notes[\"pitch\"])\n",
        "sample_note_names[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqXGfIzpuyC9"
      },
      "outputs": [],
      "source": [
        "#Plotting\n",
        "def plot_piano_roll(notes: pd.DataFrame, count: Optional[int] = None):\n",
        "  if count:\n",
        "    title = f'First {count} notes'\n",
        "  else:\n",
        "    title = f'Whole track'\n",
        "    count = len(notes['pitch'])\n",
        "  plt.figure(figsize=(20, 4))\n",
        "  plot_pitch = np.stack([notes['pitch'], notes['pitch']], axis=0)\n",
        "  plot_start_stop = np.stack([notes['start'], notes['end']], axis=0)\n",
        "  plt.plot(plot_start_stop[:, :count], plot_pitch[:, :count], color=\"b\", marker=\".\")\n",
        "  plt.xlabel('Time [s]')\n",
        "  plt.ylabel('Pitch')\n",
        "  _ = plt.title(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9ZzmHNhvVLF"
      },
      "outputs": [],
      "source": [
        "#plot_piano_roll(raw_notes, count=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5v9yvfzvmdh"
      },
      "outputs": [],
      "source": [
        "#plot_piano_roll(raw_notes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwx9KdBzvtjY"
      },
      "outputs": [],
      "source": [
        "def plot_distributions(notes: pd.DataFrame, drop_percentile=2.5):\n",
        "  plt.figure(figsize=[15, 5])\n",
        "  plt.subplot(1, 3, 1)\n",
        "  sns.histplot(notes, x=\"pitch\", bins=20)\n",
        "\n",
        "  plt.subplot(1, 3, 2)\n",
        "  max_step = np.percentile(notes['step'], 100 - drop_percentile)\n",
        "  sns.histplot(notes, x=\"step\", bins=np.linspace(0, max_step, 21))\n",
        "\n",
        "  plt.subplot(1, 3, 3)\n",
        "  max_duration = np.percentile(notes['duration'], 100 - drop_percentile)\n",
        "  sns.histplot(notes, x=\"duration\", bins=np.linspace(0, max_duration, 21))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38m5v5jewN-7"
      },
      "outputs": [],
      "source": [
        "#plot_distributions(raw_notes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLHY7vYgwTz8"
      },
      "outputs": [],
      "source": [
        "#Create MIDI File\n",
        "def notes_to_midi(notes: pd.DataFrame, out_file: str, instrument_name: str, velocity: int = 100) -> pretty_midi.PrettyMIDI:\n",
        "  pm = pretty_midi.PrettyMIDI()\n",
        "  instrument = pretty_midi.Instrument(program=pretty_midi.instrument_name_to_program(instrument_name))\n",
        "  prev_start = 0\n",
        "  for i, note in notes.iterrows():\n",
        "    start = float(prev_start + note['step'])\n",
        "    end = float(start + note['duration'])\n",
        "    note = pretty_midi.Note(velocity=velocity, pitch=int(note['pitch']), start=start, end=end)\n",
        "    instrument.notes.append(note)\n",
        "    prev_start = start\n",
        "  #Multiple Instruments Add Below To Loop?\n",
        "  pm.instruments.append(instrument)\n",
        "  pm.write(out_file)\n",
        "  return pm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB4R2M5BxcJT"
      },
      "outputs": [],
      "source": [
        "#example_file = 'example.midi'\n",
        "#example_pm = notes_to_midi(raw_notes, out_file=example_file, instrument_name=instrument_name)\n",
        "#display_audio(example_pm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mF9XoaGyUeH"
      },
      "outputs": [],
      "source": [
        "#Create Training Dataset\n",
        "num_files = 10\n",
        "all_notes = []\n",
        "for f in filenames[0:num_files]:\n",
        "  notes = midi_to_notes(f)\n",
        "  all_notes.append(notes)\n",
        "\n",
        "all_notes = pd.concat(all_notes)\n",
        "\n",
        "n_notes = len(all_notes)\n",
        "print(\"Number of Parsed Notes:\", n_notes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJnXriiOyv8K"
      },
      "outputs": [],
      "source": [
        "#Create Dataset from Notes\n",
        "key_order = [\"pitch\", \"step\", \"duration\"]\n",
        "train_notes = np.stack([all_notes[key] for key in key_order], axis=1)\n",
        "\n",
        "notes_ds = tf.data.Dataset.from_tensor_slices(train_notes)\n",
        "notes_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sequences\n",
        "\n",
        "The network is trained on sequences of notes, followed by a \"next note\" so that, given a sequence, it has a better idea of what the next pitch, step, and duration should be."
      ],
      "metadata": {
        "id": "gOBd_27F5TZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTUG_3CO3qME"
      },
      "outputs": [],
      "source": [
        "#Create Sequences so Net Will Train to Predict Next Note in Sequence\n",
        "def create_sequences(dataset: tf.data.Dataset, seq_length: int, vocab_size = 128) -> tf.data.Dataset:\n",
        "  #Returns TF Dataset of sequence and label examples\n",
        "  seq_length = seq_length+1\n",
        "  #Take 1 extra for the labels\n",
        "  windows = dataset.window(seq_length, shift=1, stride=1, drop_remainder=True)\n",
        "  # `flat_map` flattens the\" dataset of datasets\" into a dataset of tensors\n",
        "  flatten = lambda x: x.batch(seq_length, drop_remainder=True)\n",
        "  sequences = windows.flat_map(flatten)\n",
        "  #Normalize note pitch\n",
        "  def scale_pitch(x):\n",
        "    x = x/[vocab_size,1.0,1.0]\n",
        "    return x\n",
        "  # Split the labels\n",
        "  def split_labels(sequences):\n",
        "    inputs = sequences[:-1]\n",
        "    labels_dense = sequences[-1]\n",
        "    labels = {key:labels_dense[i] for i,key in enumerate(key_order)}\n",
        "    return scale_pitch(inputs), labels\n",
        "\n",
        "  return sequences.map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeT4mJXx56pG"
      },
      "outputs": [],
      "source": [
        "seq_length = 25\n",
        "vocab_size = 128\n",
        "seq_ds = create_sequences(notes_ds, seq_length, vocab_size)\n",
        "seq_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrMM22Sp6P3Z"
      },
      "outputs": [],
      "source": [
        "for seq, target in seq_ds.take(1):\n",
        "  print(\"Sequence Shape:\", seq.shape)\n",
        "  print(\"Sequence Elements (First 10):\", seq[0:10])\n",
        "  print()\n",
        "  print(\"Target:\", target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Esd_ASBX60IL"
      },
      "outputs": [],
      "source": [
        "#Batch Examples and Configure Dataset for Performance\n",
        "batch_size = 64\n",
        "buffer_size = n_notes - seq_length  # the number of items in the dataset\n",
        "train_ds = (seq_ds.shuffle(buffer_size).batch(batch_size, drop_remainder=True).cache().prefetch(tf.data.experimental.AUTOTUNE))\n",
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcBbHpWm7f5z"
      },
      "outputs": [],
      "source": [
        "#Create And Train Model\n",
        "\n",
        "#Encourage Model to Output Positive Values\n",
        "def mse_with_positive_pressure(y_true: tf.Tensor, y_pred: tf.Tensor):\n",
        "  mse = (y_true - y_pred) ** 2\n",
        "  positive_pressure = 10 * tf.maximum(-y_pred, 0.0)\n",
        "  return tf.reduce_mean(mse + positive_pressure)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Architecture\n",
        "\n",
        "Numerous architectures were tested, including different amounts of stacked LSTM units, GRU units, an RNN with LSTM layers separated by residual blocks, and even a far more simple feed-forward net with dense layers. Our peak performance was achieved with just 2 stacked LSTM layers as you see below."
      ],
      "metadata": {
        "id": "35Etfnz654D4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWaOtx9qQD9m"
      },
      "outputs": [],
      "source": [
        "#Build & Compile Model\n",
        "input_shape = (seq_length, 3)\n",
        "learning_rate = 0.005\n",
        "\n",
        "inputs = tf.keras.Input(input_shape)\n",
        "x = tf.keras.layers.LSTM(256, return_sequences=True)(inputs)\n",
        "x = tf.keras.layers.LSTM(256)(x)\n",
        "\n",
        "outputs = {\n",
        "  'pitch': tf.keras.layers.Dense(128, name='pitch')(x),\n",
        "  'step': tf.keras.layers.Dense(1, name='step')(x),\n",
        "  'duration': tf.keras.layers.Dense(1, name='duration')(x),\n",
        "}\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "loss = {\n",
        "      'pitch': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      'step': mse_with_positive_pressure,\n",
        "      'duration': mse_with_positive_pressure,\n",
        "}\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model,show_shapes=True,expand_nested=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mW8TgkjQmmj"
      },
      "outputs": [],
      "source": [
        "losses = model.evaluate(train_ds, return_dict=True)\n",
        "losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IATZ8jYcRMIf"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=loss, loss_weights={'pitch': 0.05, 'step': 1.0, 'duration':1.0}, optimizer=optimizer)\n",
        "model.evaluate(train_ds, return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXLe8ZujRgl9"
      },
      "outputs": [],
      "source": [
        "#Train Model\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath='./training_checkpoints/ckpt_{epoch}', save_weights_only=True), tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, verbose=1, restore_best_weights=True)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA0fKAk0SHQ_"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "epochs = 75\n",
        "#history = model.fit(train_ds, epochs=epochs, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Pre-Trained Weights\n",
        "!wget -O weightsRNN.zip https://www.dropbox.com/s/zhuzqmjicbkdunv/weightsRNN.zip?dl=0\n",
        "!unzip weightsRNN.zip\n",
        "model.load_weights(\"weightsRNN/weights\")\n",
        "#model.save_weights(\"./weightsRNN/weights\")"
      ],
      "metadata": {
        "id": "xHG94USUTOK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u-MT5KRTUUw"
      },
      "outputs": [],
      "source": [
        "#Plot\n",
        "#plt.plot(history.epoch, history.history['loss'], label='total loss')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rRzANgTTXLq"
      },
      "outputs": [],
      "source": [
        "#Generate Notes Definition\n",
        "def predict_next_note(notes: np.ndarray, keras_model: tf.keras.Model, temperature: float = 1.0) -> int:\n",
        "  #Generates Note ID's Using a Trained Sequence Model\n",
        "\n",
        "  assert temperature > 0\n",
        "\n",
        "  # Add batch dimension\n",
        "  inputs = tf.expand_dims(notes, 0)\n",
        "\n",
        "  predictions = model.predict(inputs)\n",
        "  pitch_logits = predictions['pitch']\n",
        "  step = predictions['step']\n",
        "  duration = predictions['duration']\n",
        "\n",
        "  pitch_logits /= temperature\n",
        "  pitch = tf.random.categorical(pitch_logits, num_samples=1)\n",
        "  pitch = tf.squeeze(pitch, axis=-1)\n",
        "  duration = tf.squeeze(duration, axis=-1)\n",
        "  step = tf.squeeze(step, axis=-1)\n",
        "\n",
        "  # `step` and `duration` values should be non-negative\n",
        "  step = tf.maximum(0, step)\n",
        "  duration = tf.maximum(0, duration)\n",
        "\n",
        "  return int(pitch), float(step), float(duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Note Generation\n",
        "\n",
        "Next we allow the net to generate its own music. The network will produce what it thinks the next pitch, step, and duration should be, and the helper function will convert these values into a playable MIDI file."
      ],
      "metadata": {
        "id": "C5mFdD4e6ekc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXYGZX0oY_lR"
      },
      "outputs": [],
      "source": [
        "#Generate Notes\n",
        "temperature = 2.0\n",
        "num_predictions = 200\n",
        "\n",
        "sample_notes = np.stack([raw_notes[key] for key in key_order], axis=1)\n",
        "\n",
        "# The initial sequence of notes; pitch is normalized similar to training\n",
        "# sequences\n",
        "input_notes = (sample_notes[:seq_length] / np.array([vocab_size, 1, 1]))\n",
        "\n",
        "generated_notes = []\n",
        "prev_start = 0\n",
        "for _ in range(num_predictions):\n",
        "  pitch, step, duration = predict_next_note(input_notes, model, temperature)\n",
        "  start = prev_start + step\n",
        "  end = start + duration\n",
        "  input_note = (pitch, step, duration)\n",
        "  generated_notes.append((*input_note, start, end))\n",
        "  input_notes = np.delete(input_notes, 0, axis=0)\n",
        "  input_notes = np.append(input_notes, np.expand_dims(input_note, 0), axis=0)\n",
        "  prev_start = start\n",
        "\n",
        "generated_notes = pd.DataFrame(generated_notes, columns=(*key_order, 'start', 'end'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcI9AgMaaWW8"
      },
      "outputs": [],
      "source": [
        "generated_notes.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzqeDHspaYxK"
      },
      "outputs": [],
      "source": [
        "out_file = 'output.mid'\n",
        "out_pm = notes_to_midi(generated_notes, out_file=out_file, instrument_name=instrument_name)\n",
        "display_audio(out_pm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEKs15XMac6h"
      },
      "outputs": [],
      "source": [
        "#Download\n",
        "from google.colab import files\n",
        "#files.download(out_file)\n",
        "#files.download(\"weightsRNN.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEk3TBCrakVB"
      },
      "outputs": [],
      "source": [
        "#Plots\n",
        "plot_piano_roll(generated_notes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwXKdRo6amXc"
      },
      "outputs": [],
      "source": [
        "plot_distributions(generated_notes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "musicGenRNN_Demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}